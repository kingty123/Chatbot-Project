{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whisper 사용\n",
    "import whisper\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "#import time\n",
    "import threading                    # 비동거 작업이나, 동시에 여러 작업 처리\n",
    "import os\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import base64\n",
    "import pyttsx3\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "#from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"base\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 환경 변수를 로드합니다.\n",
    "load_dotenv()\n",
    "\n",
    "# API 키를 환경 변수에서 가져옵니다.\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = API_KEY \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "Created a chunk of size 617, which is longer than the specified 600\n",
      "Created a chunk of size 699, which is longer than the specified 600\n",
      "Created a chunk of size 2225, which is longer than the specified 600\n"
     ]
    }
   ],
   "source": [
    "# Langchain을 활용하기 위한 설정과 RAG 설정을 진행합니다.\n",
    "llm = ChatOpenAI(model='gpt-4',\n",
    "    temperature=0.7)\n",
    "cache_dir = LocalFileStore(\"./.cache/practice/\")\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "#retriever = FAISS.load_local(\"./refer.txt\", OpenAIEmbeddings(), allow_dangerous_deserialization=True)\n",
    "\n",
    "\n",
    "loader = UnstructuredFileLoader(\"refer.txt\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 15:29:46.491 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-18 15:29:46.493 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-18 15:29:46.495 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-18 15:29:46.498 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-18 15:29:46.499 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-18 15:29:46.500 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-18 15:29:46.503 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-18 15:29:46.504 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-18 15:29:46.505 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-18 15:29:46.506 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-18 15:29:46.506 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-18 15:29:46.508 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-18 15:29:46.508 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-18 15:29:46.514 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-18 15:29:46.514 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-18 15:29:46.514 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-18 15:29:46.514 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-18 15:29:46.514 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-18 15:29:46.514 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function Halo.__init__.<locals>.clean_up at 0x000001F22E02CC20> (for post_run_cell), with arguments args (<ExecutionResult object at 1f22e135ad0, execution_count=22 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 1f22c3fff50, raw_cell=\"#from streamlit_geolocation import streamlit_geolo..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/c%3A/Users/INO04/Downloads/RealtimeSTT-master/temp.ipynb#X11sZmlsZQ%3D%3D> result=None>,),kwargs {}:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function Halo.__init__.<locals>.clean_up at 0x000001F22E02DB20> (for post_run_cell), with arguments args (<ExecutionResult object at 1f22e135ad0, execution_count=22 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 1f22c3fff50, raw_cell=\"#from streamlit_geolocation import streamlit_geolo..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/c%3A/Users/INO04/Downloads/RealtimeSTT-master/temp.ipynb#X11sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function Halo.__init__.<locals>.clean_up at 0x000001F22E02CF40> (for post_run_cell), with arguments args (<ExecutionResult object at 1f22e135ad0, execution_count=22 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 1f22c3fff50, raw_cell=\"#from streamlit_geolocation import streamlit_geolo..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/c%3A/Users/INO04/Downloads/RealtimeSTT-master/temp.ipynb#X11sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function Halo.__init__.<locals>.clean_up at 0x000001F22DE48680> (for post_run_cell), with arguments args (<ExecutionResult object at 1f22e135ad0, execution_count=22 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 1f22c3fff50, raw_cell=\"#from streamlit_geolocation import streamlit_geolo..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/c%3A/Users/INO04/Downloads/RealtimeSTT-master/temp.ipynb#X11sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function Halo.__init__.<locals>.clean_up at 0x000001F22E02D620> (for post_run_cell), with arguments args (<ExecutionResult object at 1f22e135ad0, execution_count=22 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 1f22c3fff50, raw_cell=\"#from streamlit_geolocation import streamlit_geolo..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/c%3A/Users/INO04/Downloads/RealtimeSTT-master/temp.ipynb#X11sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function Halo.__init__.<locals>.clean_up at 0x000001F22DD63740> (for post_run_cell), with arguments args (<ExecutionResult object at 1f22e135ad0, execution_count=22 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 1f22c3fff50, raw_cell=\"#from streamlit_geolocation import streamlit_geolo..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/c%3A/Users/INO04/Downloads/RealtimeSTT-master/temp.ipynb#X11sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function Halo.__init__.<locals>.clean_up at 0x000001F22E02E980> (for post_run_cell), with arguments args (<ExecutionResult object at 1f22e135ad0, execution_count=22 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 1f22c3fff50, raw_cell=\"#from streamlit_geolocation import streamlit_geolo..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/c%3A/Users/INO04/Downloads/RealtimeSTT-master/temp.ipynb#X11sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function Halo.__init__.<locals>.clean_up at 0x000001F18409C680> (for post_run_cell), with arguments args (<ExecutionResult object at 1f22e135ad0, execution_count=22 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 1f22c3fff50, raw_cell=\"#from streamlit_geolocation import streamlit_geolo..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/c%3A/Users/INO04/Downloads/RealtimeSTT-master/temp.ipynb#X11sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function Halo.__init__.<locals>.clean_up at 0x000001F22E02E700> (for post_run_cell), with arguments args (<ExecutionResult object at 1f22e135ad0, execution_count=22 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 1f22c3fff50, raw_cell=\"#from streamlit_geolocation import streamlit_geolo..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/c%3A/Users/INO04/Downloads/RealtimeSTT-master/temp.ipynb#X11sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "#from streamlit_geolocation import streamlit_geolocation\n",
    "\n",
    "\n",
    "# Streamlit 세션 상태를 초기화합니다. 이는 대화 내역을 저장하는 데 사용됩니다.\n",
    "if 'chat_history' not in st.session_state:\n",
    "    st.session_state.chat_history = []\n",
    "if 'question' not in st.session_state:\n",
    "    st.session_state.question = \"\"\n",
    "if 'mode' not in st.session_state:\n",
    "    st.session_state.mode = \"voice\" # 기본 모드 = 음성\n",
    "\n",
    "# 이미지 경로 설정\n",
    "ai_avatar = \"karina_1-removebg-preview.png\"  # AI 아바타 이미지\n",
    "user_avatar = \"사람이미지_1.jpg\"  # 사용자 아바타 이미지\n",
    "\n",
    "# 이미지 Base64 변환 함수\n",
    "def get_image_base64(image_path):\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        return base64.b64encode(img_file.read()).decode()\n",
    "\n",
    "\n",
    "\n",
    "# 사용자 질문에 대한 응답을 처리하는 함수입니다.\n",
    "def ask_gpt(user_question):\n",
    "    # 이전 대화 내역을 기반으로 CHATGPT에게 요청할 쿼리를 생성합니다.\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            당신은 'Selena'라는 차량용 인공지능 비서입니다. 친근하고 유머러스하게 사용자의 질문에 답변하며, 운전 중에도 안전하고 유용한 정보를 제공하는 것이 주된 역할입니다. 사용자가 질문을 하면 정확하고 명확한 답변을 주되, 운전 중에도 편안하게 이해할 수 있도록 간결하고 친근하게 응답해주세요. 아래 지침을 따라주세요:\n",
    "\n",
    "            1. 사용자의 질문을 신중하게 분석합니다.\n",
    "            2. 명확하고 간결하며, 운전 중에도 쉽게 이해할 수 있도록 답변합니다.\n",
    "            3. 질문이 복잡하면 이해하기 쉬운 작은 부분으로 나누어 설명합니다.\n",
    "            4. 예시나 비유를 사용하여 개념을 쉽게 풀어 설명합니다.\n",
    "            5. 답변을 잘 모르겠다면 그 사실을 인정하고, 관련된 정보를 찾을 수 있는 방법을 제시합니다.\n",
    "            6. 사용자가 더 알아볼 수 있도록 후속 질문을 유도하거나 관련된 주제를 제안합니다.\n",
    "            7. 항상 긍정적이고 격려하는 태도를 유지합니다.\n",
    "            8. 운전 중에는 안전을 최우선으로 고려하여 대화를 유도합니다.\n",
    "            \\n\\n\n",
    "            {context}\",\n",
    "            \"\"\"\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "    )\n",
    "    result = chain.invoke(user_question)\n",
    "        \n",
    "    return result.content\n",
    "\n",
    "\n",
    "\n",
    "# 질문 입력 클리어 함수\n",
    "def clear_input():\n",
    "    st.session_state.question = \"\"\n",
    "\n",
    "\n",
    "# 음성을 합성하는 함수 : TTS\n",
    "def speak(text):\n",
    "    engine = pyttsx3.init()\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "from RealtimeSTT import AudioToTextRecorder\n",
    "import pyautogui\n",
    "\n",
    "\n",
    "# # 음성 인식 콜백 함수\n",
    "# def recognize_audio(indata, frames, time, status):\n",
    "#     if status:\n",
    "#         print(status)\n",
    "\n",
    "#     # numpy 배열을 바이트로 변환\n",
    "#     audio = np.frombuffer(indata, dtype=np.float32)\n",
    "\n",
    "#     # 이름 호명하면 STT 시작\n",
    "#     if detect_keyword(audio):\n",
    "#         print(\"안녕하세요. 무엇을 도와드릴까요?😊\")\n",
    "#         # 별도의 스레드에서 명령을 듣고 처리\n",
    "#         threading.Thread(target=process_command).start()\n",
    "#         # 동기처리\n",
    "#         # process_command()\n",
    "\n",
    "\n",
    "# 키워드 감지 함수(기본 예시)\n",
    "def detect_keyword(audio):\n",
    "    #return np.random.rand() > 0.97  # 3% 확률로 키워드 감지\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "    mel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)\n",
    "    result = model.transcribe(mel)\n",
    "\n",
    "    # 감지된 텍스트에서 \"Hey\"를 찾음\n",
    "    if \"Selena\" in result['text'].lower():\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# # 사용자의 명령을 듣는 함수\n",
    "# def listen_for_command():\n",
    "#     print(\"잠시만 기다려주세요 😊\")\n",
    "#     time.sleep(10)  # 사용자에게 말을 할 시간을 줌\n",
    "#     recording = sd.rec(int(5 * 44100), samplerate=44100, channels=1, dtype='float32')\n",
    "#     sd.wait()  # 녹음 완료까지 대기\n",
    "#     return recording\n",
    "\n",
    "\n",
    "# # 오디오를 텍스트로 변환하는 함수\n",
    "# def transcribe_audio(audio):\n",
    "#     audio = whisper.pad_or_trim(audio)\n",
    "#     mel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)\n",
    "#     result = model.transcribe(mel)\n",
    "#     return result['text']\n",
    "\n",
    "\n",
    "# # process_command() : 특정 키워드를 감지했을 때 호출되며, 사용자의 음성 명령 처리\n",
    "# def process_command():\n",
    "#     audio = listen_for_command()            # 사용자가 음성으로 명령을 입력할 수 있도록 대기기\n",
    "#     command = transcribe_audio(audio)       # 음성 -> 텍스트\n",
    "#     print(\"사용자 요구사항 : {command}\")        # 사용자 요구사항 출력\n",
    "#     # print(f\"사용자 요구사항: {command}\")\n",
    "#     response = respond_to_command(command)             # 유저 요구사항에 따라 적절한 응답 제공\n",
    "#     print(response)\n",
    "#     pyautogui.typewrite(command)            # 텍스트(사용자 명령)를 채팅창에 입력\n",
    "\n",
    "\n",
    "# process_command() : 특정 키워드를 감지했을 때 호출되며, 사용자의 음성 명령 처리\n",
    "def process_command(text):\n",
    "    print(f\"사용자 요구사항: {text}\")\n",
    "    response = respond_to_command(text)\n",
    "    print(response)\n",
    "    pyautogui.typewrite(response)           # 응답을 채팅창에 입력\n",
    "\n",
    "\n",
    "# 수정 필요 : 명령에 대한 응답을 제공하는 함수\n",
    "def respond_to_command(command):\n",
    "    # 예시: 특정 명령어에 대한 응답 처리\n",
    "    if \"날씨\" in command:\n",
    "        response = \"오늘의 날씨는 맑습니다.☀️\"\n",
    "    elif \"시간\" in command:\n",
    "        response = \"현재 시간은 3시입니다.🕒\"\n",
    "    else:\n",
    "        response = \"요청하신 내용을 이해하지 못했습니다. 다시 한 번 말씀해주세요 🥲\"\n",
    "    return response\n",
    "\n",
    "\n",
    "def listen_for_audio():\n",
    "    recorder = AudioToTextRecorder()\n",
    "    while True:\n",
    "        recorder.text(process_command)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # streamlit page configuration\n",
    "    st.set_page_config(layout=\"wide\", page_title=\"Wellcome, I'm Selena\")\n",
    "    # 페이지 제목\n",
    "    st.title('Selena')\n",
    "\n",
    "    # CSS 스타일 정의\n",
    "    st.markdown(\"\"\"\n",
    "    <style>\n",
    "    .chat-container {\n",
    "        display: flex;\n",
    "        align-items: start;\n",
    "        margin-bottom: 20px;\n",
    "        width: 100%;\n",
    "    }\n",
    "\n",
    "    .chat-image {\n",
    "        width: 100px;\n",
    "        height: 150px;\n",
    "        border-radius: 10%;\n",
    "        margin: 0 15px;\n",
    "        object-fit : cover;\n",
    "    }\n",
    "    .chat-image.ai {\n",
    "        flex-direction: row-reverse;\n",
    "        align: right;\n",
    "        align-items: flex-start;\n",
    "        justify-content: flex-end;\n",
    "    }\n",
    "    .chat-message {\n",
    "        background-color: #f0f2f6;\n",
    "        padding: 10px;\n",
    "        border-radius: 10px;\n",
    "        max-width: 80%;\n",
    "    }\n",
    "\n",
    "    .chat-container.ai {\n",
    "        flex-direction: row-reverse;\n",
    "        text-align: auto;\n",
    "    }         \n",
    "                \n",
    "    .chat-message.ai {\n",
    "        margin-right: 0;\n",
    "        text-align: auto;\n",
    "    }   \n",
    "\n",
    "    </style>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "\n",
    "    # 음성 및 채팅모드 선택\n",
    "    # mode = st.radio(\"Please choose 1 of them : \", [\"voice\", \"chat\"])\n",
    "    # st.session_state.mode = mode\n",
    "    question = st.text_input(\"언제든 편하게 물어보세요\", value=st.session_state.question, key=\"user_input\")\n",
    "\n",
    "\n",
    "    # 질문에 대한 답변을 생성하는 버튼\n",
    "    if question and (st.button('답변') or question != st.session_state.get('previous_question', '')):\n",
    "        st.session_state['previous_question'] = question\n",
    "        answer = ask_gpt(question)  # GPT-3 모델을 호출하여 답변을 받습니다.\n",
    "\n",
    "        st.session_state.chat_history.append(f\"Question: {question}\")\n",
    "        st.session_state.chat_history.append(f\"Answer: {answer}\")\n",
    "        st.session_state.question = \"\"  # 입력 필드 클리어\n",
    "\n",
    "        # # 음성 모드일 경우 음성으로 답변 읽기\n",
    "        # if st.session_state.mode == \"voice\":\n",
    "        #     speak(answer)\n",
    "\n",
    "            \n",
    "        # 대화 내역 표시\n",
    "        for message in st.session_state.chat_history:\n",
    "            if message.startswith(\"Question:\"):\n",
    "            # 사용자와 ai 메세지 구분\n",
    "            #user_message = st.session_state.chat_history[i]\n",
    "            #ai_message = st.session_state.chat_history[i + 1]\n",
    "            \n",
    "                # 사용자 메시지 컨테이너\n",
    "                st.markdown(\n",
    "                    f\"\"\"\n",
    "                    <div class=\"chat-container\">\n",
    "                        <img src=\"data:image/jpeg;base64,{get_image_base64(user_avatar)}\" class=\"chat-image\">\n",
    "                        <div class=\"chat-message\">{message}</div>\n",
    "                        <div class=\"button-group\">\n",
    "                            <button class=\"copy-btn\" onclick=\"copyToClipboard('{message}')\"> ✔️ </button>\n",
    "                            <button class=\"eval-btn\" onclick=\"evaluateResponse('Good')\"> 👍 </button>\n",
    "                            <button class=\"eval-btn\" onclick=\"evaluateResponse('Bad')\"> 👎 </button>\n",
    "                        </div>\n",
    "                    </div>\n",
    "                    \"\"\",\n",
    "                    unsafe_allow_html=True\n",
    "                )\n",
    "\n",
    "            else : \n",
    "                # ai 답변 컨테이너\n",
    "                st.markdown(\n",
    "                    f\"\"\"\n",
    "                    <div class=\"chat-container ai\">\n",
    "                        <img src=\"data:image/jpeg;base64,{get_image_base64(ai_avatar)}\" class=\"chat-image\">\n",
    "                        <div class=\"chat-message ai\">{message}</div>\n",
    "                        <div class=\"button-group\">\n",
    "                            <button class=\"copy-btn\" onclick=\"copyToClipboard('{message}')\"> ✔️ </button>\n",
    "                            <button class=\"eval-btn\" onclick=\"evaluateResponse('Good')\"> 👍 </button>\n",
    "                            <button class=\"eval-btn\" onclick=\"evaluateResponse('Bad')\"> 👎 </button>    \n",
    "                        </div>\n",
    "                    </div>\n",
    "                    \"\"\", \n",
    "                    unsafe_allow_html=True\n",
    "                )\n",
    "\n",
    "        # 자동 스크롤 아래로\n",
    "        st.markdown(\"<script>window.scrollTo(0, document.body.scrollHeight);</script>\", unsafe_allow_html=True)\n",
    "        # 자동 스크롤 위로 (예: 100px 위로 이동)\n",
    "        st.markdown(\"<script>window.scrollBy(0, -100);</script>\", unsafe_allow_html=True)\n",
    "\n",
    "            \n",
    "    else:\n",
    "        st.error(\"Please enter a question.\")\n",
    "\n",
    "\n",
    "\n",
    "    # 자바스크립트 함수 추가 (복사 및 평가 기능)\n",
    "    st.markdown(\"\"\"\n",
    "    <div id=\"scroll-top\" onclick=\"window.scrollTo(0,0)\">⬆️</div>\n",
    "    <div id=\"scroll-bottom\" onclick=\"window.scrollTo(0,document.body.scrollHeight)\">⬇️</div>\n",
    "    <script>\n",
    "        function copyToClipboard(text) {\n",
    "            navigator.clipboard.writeText(text).then(() => {\n",
    "                alert('Copied to clipboard');\n",
    "            });\n",
    "        }\n",
    "\n",
    "        function evaluateResponse(evaluation) {\n",
    "            alert('You rated this response as: ' + evaluation);\n",
    "        }\n",
    "    </script>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "\n",
    "\n",
    "#    .        .       .   .     \n",
    "\n",
    "        # recorder = AudioToTextRecorder()\n",
    "        # while True:\n",
    "        #      recorder.text(process_command)         # 사용자가 음성을 입력할 때 마다, 해당 코드가 호출되어 process_command 실행\n",
    "    \n",
    "    # # 실시간 오디오 스트림 시작\n",
    "    # with sd.RawInputStream(callback=recognize_audio, channels=1, samplerate=44100, dtype='float32'):\n",
    "    #     print(\"Listening...\")\n",
    "    #     sd.sleep(-1)  # 무한 대기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Velog 추천 방법\n",
    "from fastapi import FastAPI, File, UploadFile, HTTPException, Body\n",
    "from fastapi.responses import JSONResponse\n",
    "import whisper\n",
    "from openai import OpenAI\n",
    "import os"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
