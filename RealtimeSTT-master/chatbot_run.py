import sys
import streamlit as st
import streamlit.components.v1 as components
import sounddevice as sd
import numpy as np
import tempfile
import os
import warnings
from gtts import gTTS                             # TTS

from openai import OpenAI
from scipy.io.wavfile import write
from dotenv import load_dotenv
import faster_whisper                           # STT

# streamlit ì„œë²„ ì„í¬íŠ¸
st.set_page_config(layout="centered", initial_sidebar_state="expanded")

# ì±„íŒ… UI CSS ìŠ¤íƒ€ì¼ ì •ì˜
css = """
    <style>
        .chat-container {
            display: flex;
            align-items: flex-start;
            margin-bottom: 20px;
            width: 100%;
        }

        .chat-image {
            width: 100px;
            height: 150px;
            border-radius: 10%;
            margin: 0 15px;
            object-fit: cover;
        }

        .chat-message {
            background-color: #f0f2f6;
            padding: 10px;
            border-radius: 10px;
            max-width: 80%;
        }

        .chat-container.ai {
            flex-direction: row-reverse;
            text-align: right;
        }     

        .chat-message.ai {
            margin-right: 0;
        }

        .scroll-buttons {
            position: fixed;
            bottom: 20px;
            right: 20px;
            display: flex;
            flex-direction: column;
            gap: 10px;
            z-index: 1000;
        }
        
        .scroll-buttons button:hover {
            background-color: #f0f0f0;
        }   
    </style>
"""

# JavaScriptë¡œ ìŠ¤í¬ë¡¤ ê¸°ëŠ¥ ì¶”ê°€
js = """
    <script>
        function scrollToTop() {
            window.scrollTo({top: 0, behavior: 'smooth'});
        }
        function scrollToBottom() {
            window.scrollTo({top: document.body.scrollHeight, behavior: 'smooth'});
        }

        var audio = document.querySelector("audio");
        if (audio) {
            audio.autoplay = true;
            audio.muted = false; 
            audio.addEventListener("ended", function() {
                console.log("Audio playback completed.");
            });
        }
    </script>
"""

# HTMLë¡œ ë²„íŠ¼ ì¶”ê°€ (ì˜¤ë¥¸ìª½ í•˜ë‹¨ì— ê³ ì •)
html = """
    <div class="scroll-buttons">
        <button onclick="scrollToTop()">â¬†ï¸</button>
        <button onclick="scrollToBottom()">â¬‡ï¸</button>
    </div>
"""

# Streamlit ì»´í¬ë„ŒíŠ¸ ìƒì„±
components.html(
    css + js + html,
    height=200,  # í•„ìš”ì— ë”°ë¼ ë†’ì´ ì¡°ì •
)

st.title("ğŸ™ï¸ SelenaAI ")

sys.stdout.reconfigure(encoding='utf-8')        # ì´ëª¨í‹°ì½˜ ì‚¬ìš© ìš©ì´
warnings.filterwarnings("ignore")
ai_img = "WOODZ_êµ°ë³µ.jpg"
user_img = "ì‚¬ëŒì´ë¯¸ì§€_1.jpg"

# Whisper ëª¨ë¸ ë¡œë“œ (ìºì‹± ì‚¬ìš©)
@st.cache_resource
def load_faster_whisper_model():
    model = faster_whisper.WhisperModel("medium", device="cpu", compute_type="int8")  # ëª¨ë¸ í¬ê¸° ë° compute_type ì¡°ì • ê°€ëŠ¥
    return model

model = load_faster_whisper_model()

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

# ì‚¬ì´ë“œë°” ìƒì„±
with st.sidebar:
    menu = {"ğŸ ": "home", "ğŸ†•": "news", "ğŸ’¬": "history"}
    page = st.radio("Menu", options=menu.keys(), format_func=lambda x: f"{x} {menu[x].capitalize()}")

    if page:
        st.session_state.page = menu[page]

    

# ì±„íŒ… ê¸°ë¡ ìœ ì§€
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []

# ìŒì„± ë…¹ìŒ í•¨ìˆ˜
def record_audio(duration=6, samplerate=44100):
    st.write("ğŸ¤ ë…¹ìŒ ì¤‘ ì…ë‹ˆë‹¤. ë§ì”€í•´ì£¼ì„¸ìš”!")
    audio_data = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=1, dtype=np.int16)
    sd.wait()
    temp_audio_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
    write(temp_audio_file.name, samplerate, audio_data)
    return temp_audio_file.name

# ìŒì„± -> í…ìŠ¤íŠ¸ : STT (faster-whisper ì‚¬ìš©)
def speech_to_text(audio_file):
    segments, info = model.transcribe(audio_file, beam_size=5)
    text = ""
    for segment in segments:
        text += segment.text
    return text

# gTTS ìŒì„± ì¶œë ¥ í•¨ìˆ˜
def speak_gtts(text):
    try:
        tts = gTTS(text=text, lang='ko', slow=False)
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as temp_file:
            tts.save(temp_file.name)
            audio_file = open(temp_file.name, "rb")
            audio_bytes = audio_file.read()
            st.audio(audio_bytes, format="audio/mp3", start_time=0)
            components.html(
                f"""
                    <script>
                        var audio = document.querySelector("audio");
                        audio.autoplay = true;
                    </script>
                """,
                height=0,
            )
    except Exception as e:
        st.error(f"âŒ gTTS ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        if 'audio_file' in locals():
            audio_file.close()

# ìµœì¢… TTS ì‹¤í–‰ í•¨ìˆ˜
def speak(text):
    speak_gtts(text)

# GPT ì‘ë‹µ ìƒì„±
def ask_gpt(user_input):
    try:
        chat_completion = client.chat.completions.create(
            messages=[
                {
                    "role": "user",
                    "content": user_input,
                }
            ],
            model="gpt-4-turbo",
        )
        return chat_completion.choices[0].message.content
    except Exception as e:
        return f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

# ìŒì„± ë…¹ìŒ ë²„íŠ¼
if st.button("ğŸ¤ SelenaAI ì…ë‹ˆë‹¤. ë¬´ì—‡ì´ë“  í¸í•˜ê²Œ ì§ˆë¬¸í•˜ì„¸ìš”"):
    audio_file = record_audio()
    st.success("âœ… ë…¹ìŒ ì™„ë£Œ! ìŒì„± ë³€í™˜ ì¤‘ ì…ë‹ˆë‹¤...")
    text_input = speech_to_text(audio_file)
    response = ask_gpt(text_input)

    with st.chat_message("user"):
        st.image(user_img, width=50)
        st.write(text_input)

    with st.chat_message("assistant"):
        st.image(ai_img, width=50)
        st.write(response)

    st.session_state.chat_history.append({"role": "user", "content": text_input})
    st.session_state.chat_history.append({"role": "assistant", "content": response})

    speak(response)
    os.remove(audio_file)

st.subheader("ğŸ’¬ ëŒ€í™” ê¸°ë¡")
for chat in st.session_state.chat_history:
    with st.chat_message(chat["role"]):
        if chat["role"] == "user":
            st.image(user_img, width=50)
        else:
            st.image(ai_img, width=50)
        st.write(chat["content"])

if st.button("ğŸ›‘ ì¢…ë£Œ"):
    st.session_state.chat_history = []
    st.success("ëŒ€í™” ê¸°ë¡ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
