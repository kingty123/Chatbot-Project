import sys
sys.stdout.reconfigure(encoding='utf-8')        # ì´ëª¨í‹°ì½˜ ì‚¬ìš© ìš©ì´

import streamlit as st
import sounddevice as sd
import numpy as np
# import whisper
import tempfile
import os
# import openai
import warnings
import pyttsx3                                  # TTS
from gtts import gTTS
#import re
from openai import OpenAI
from scipy.io.wavfile import write
from dotenv import load_dotenv
import faster_whisper                           # STT
#from io import BytesIO

# streamlit ì„œë²„ ì„í¬íŠ¸
st.set_page_config(layout="centered", initial_sidebar_state="expanded")


warnings.filterwarnings("ignore")
ai_img = "WOODZ_êµ°ë³µ.jpg"
user_img = "ì‚¬ëŒì´ë¯¸ì§€_1.jpg"


# Whisper ëª¨ë¸ ë¡œë“œ (ìºì‹± ì‚¬ìš©)
@st.cache_resource
def load_faster_whisper_model():
    model = faster_whisper.WhisperModel("medium", device="cpu", compute_type="int8")  # ëª¨ë¸ í¬ê¸° ë° compute_type ì¡°ì • ê°€ëŠ¥
    return model

model = load_faster_whisper_model()

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
#openai.api_key = os.environ['OPENAI_API_KEY']
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

st.title("ğŸ™ï¸ SelenaAI ")

# ì‚¬ì´ë“œë°” ìƒì„±
with st.sidebar.container():
    # ì‚¬ì´ë“œë°” ì˜µì…˜
    # ì†ë„ ì¡°ì ˆ ìŠ¬ë¼ì´ë”
    tts_speed = st.slider("ìŒì„± ì†ë„ ì¡°ì ˆ", min_value=100, max_value=600, value=180, step=5)
    
    # ì‚¬ìš©ìê°€ ì„ íƒí•  ìˆ˜ ìˆëŠ” TTS ì—”ì§„ ëª©ë¡
    tts_engine = st.selectbox(
        "ì‚¬ìš©í•  TTS ì—”ì§„ì„ ì„ íƒí•˜ì„¸ìš”", 
        ["gTTS (ì˜¨ë¼ì¸)", "pyttsx3 (ì˜¤í”„ë¼ì¸)"])

    # ì„¤ì • ì €ì¥
    st.session_state.tts_engine = tts_engine
    st.session_state.tts_speed = tts_speed



# ì±„íŒ… ê¸°ë¡ ìœ ì§€
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []


# ìŒì„± ë…¹ìŒ í•¨ìˆ˜
def record_audio(duration=6, samplerate=44100):
    st.write("ğŸ¤ ë…¹ìŒ ì¤‘ ì…ë‹ˆë‹¤. ë§ì”€í•´ì£¼ì„¸ìš”!")
    audio_data = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=1, dtype=np.int16)
    sd.wait()
    temp_audio_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
    write(temp_audio_file.name, samplerate, audio_data)
    return temp_audio_file.name


# # ìŒì„± -> í…ìŠ¤íŠ¸ : STT
# def speech_to_text(audio_file):
#     audio = whisper.load_audio(audio_file)
#     result = model.transcribe(audio)
#     return result["text"]

# ìŒì„± -> í…ìŠ¤íŠ¸ : STT (faster-whisper ì‚¬ìš©)
def speech_to_text(audio_file):
    segments, info = model.transcribe(audio_file, beam_size=5)
    text = ""
    for segment in segments:
        text += segment.text
    return text


# í…ìŠ¤íŠ¸ -> ìŒì„± : TTS(pyttsx3)
def speak_pyttsx3(text):
    if not text:
        return  # ë¹ˆ í…ìŠ¤íŠ¸ëŠ” ë³€í™˜í•˜ì§€ ì•ŠìŒ
    
    engine = pyttsx3.init()
    voices = engine.getProperty('voices')

    # í•œêµ­ì–´ ìŒì„± ì„ íƒ
    for voice in voices:
        if "Heami" in voice.name or "korean" in voice.name.lower():
            engine.setProperty('voice', voice.id)
            break

    engine.setProperty('rate', tts_speed)
    engine.say(text)
    engine.runAndWait()


# gTTS ìŒì„± ì¶œë ¥ í•¨ìˆ˜
def speak_gtts(text):
    try:
        tts = gTTS(text=text, lang='ko', slow=False)
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as temp_file:
            tts.save(temp_file.name)
            st.audio(temp_file.name, format="audio/mp3")
    except Exception as e:
        st.error(f"âŒ gTTS ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        if 'temp_file' in locals() and os.path.exists(temp_file.name):
            os.remove(temp_file.name)


# ìµœì¢… TTS ì‹¤í–‰ í•¨ìˆ˜
def speak(text):
    if tts_engine == "pyttsx3 (ì˜¤í”„ë¼ì¸)":
        speak_pyttsx3(text)
    else:
        speak_gtts(text)


# GPT ì‘ë‹µ ìƒì„±
def ask_gpt(user_input):
    try:
        chat_completion = client.chat.completions.create(
            messages=[
                {
                    "role": "user",
                    "content": user_input,
                }
            ],
            model="gpt-4-turbo",
        )
        return chat_completion.choices[0].message.content
    except Exception as e:
        return f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"



# ìŒì„± ë…¹ìŒ ë²„íŠ¼
if st.button("ğŸ¤ SelenaAI ì…ë‹ˆë‹¤. ë¬´ì—‡ì´ë“  í¸í•˜ê²Œ ì§ˆë¬¸í•˜ì„¸ìš”"):
    audio_file = record_audio()
    st.success("âœ… ë…¹ìŒ ì™„ë£Œ! ìŒì„± ë³€í™˜ ì¤‘ ì…ë‹ˆë‹¤...")
    text_input = speech_to_text(audio_file)
    # st.write(f"ğŸ“ {text_input}")
    response = ask_gpt(text_input)

    col1, col2 = st.columns(2)
    
    # ì‚¬ìš©ì ì´ë¯¸ì§€ì™€ í•¨ê»˜ ë©”ì‹œì§€ë¥¼ ì™¼ìª½ì— ì¶œë ¥
    with col1:
        with st.chat_message("user"):
            st.image(user_img, width=50)
            st.write(text_input)


    # response = ask_gpt(text_input)
    
    
    # ì±—ë´‡ ì´ë¯¸ì§€ì™€ í•¨ê»˜ ë©”ì‹œì§€ë¥¼ ì˜¤ë¥¸ìª½ì— ì¶œë ¥
    with col2:
        with st.chat_message("assistant"):
            st.image(ai_img, width=50)
            st.write(response)


    st.session_state.chat_history.append({"role": "user", "content": text_input})
    st.session_state.chat_history.append({"role": "assistant", "content": response})
    
    speak(response)
    os.remove(audio_file)


st.subheader("ğŸ’¬ ëŒ€í™” ê¸°ë¡")
for chat in st.session_state.chat_history:
    # role = "ğŸ—£ï¸ ì‚¬ìš©ì" if chat["role"] == "user" else "ğŸ¤– ì±—ë´‡"
    # st.write(f"**{role}**: {chat['content']}")

    with st.chat_message(chat["role"]):
        if chat["role"] == "user":
            st.image(user_img, width=50)
        else:
            st.image(ai_img, width=50)
        st.write(chat["content"])


# ìŠ¤í¬ë¡¤ ë²„íŠ¼ ì¶”ê°€ (ë²„íŠ¼ì„ ì˜¤ë¥¸ìª½ í•˜ë‹¨ì— ê³ ì •)
scroll_css = """
    <style>
        .scroll-buttons {
            position: fixed;
            bottom: 20px;
            right: 20px;
            display: flex;
            flex-direction: column;
            gap: 10px;
            z-index: 1000;
        }
        .scroll-buttons button {
            width: 50px;
            height: 50px;
            font-size: 20px;
            border-radius: 50%;
            border: none;
            background-color: #ffffff;
            box-shadow: 2px 2px 10px rgba(0, 0, 0, 0.2);
            cursor: pointer;
        }
        .scroll-buttons button:hover {
            background-color: #f0f0f0;
        }
    </style>
"""
st.markdown(scroll_css, unsafe_allow_html=True)


# JavaScriptë¡œ ìŠ¤í¬ë¡¤ ê¸°ëŠ¥ ì¶”ê°€
scroll_js = """
    <script>
        function scrollToTop() {
            window.scrollTo({top: 0, behavior: 'smooth'});
        }
        function scrollToBottom() {
            window.scrollTo({top: document.body.scrollHeight, behavior: 'smooth'});
        }
    </script>
"""
st.markdown(scroll_js, unsafe_allow_html=True)


# HTMLë¡œ ë²„íŠ¼ ì¶”ê°€ (ì˜¤ë¥¸ìª½ í•˜ë‹¨ì— ê³ ì •)
scroll_buttons_html = """
    <div class="scroll-buttons">
        <button onclick="scrollToTop()">â¬†ï¸</button>
        <button onclick="scrollToBottom()">â¬‡ï¸</button>
    </div>
"""
st.markdown(scroll_buttons_html, unsafe_allow_html=True)



if st.button("ğŸ›‘ ì¢…ë£Œ"):
    st.session_state.chat_history = []
    st.success("ëŒ€í™” ê¸°ë¡ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
