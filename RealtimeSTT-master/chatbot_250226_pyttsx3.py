import sys
sys.stdout.reconfigure(encoding='utf-8')        # ì´ëª¨í‹°ì½˜ ì‚¬ìš© ìš”ì´
#sys.path.append('thrid_party/Matcha-TTS')       # CosyVoice2-0.5B ì‚¬ìš©

import streamlit as st
import sounddevice as sd
import numpy as np
import whisper
import tempfile
import os
import openai
import warnings
import pyttsx3
from gtts import gTTS
#import re
from scipy.io.wavfile import write
from dotenv import load_dotenv
#from io import BytesIO


st.set_page_config(layout="centered", initial_sidebar_state="expanded")
# from cosyvoice.cli.cosyvoice import CosyVoice2
# from cosyvoice.utils.file_utils import load_wav
# import torchaudio

warnings.filterwarnings("ignore")
ai_img = "WOODZ_êµ°ë³µ.jpg"
user_img = "ì‚¬ëŒì´ë¯¸ì§€_1.jpg"


# # CosyVoice2 ì‚¬ìš© : JIT(ìµœì í™”ëœ ì‹¤í–‰), TensorRT, FP16(ë°˜ì •ë°€ë„ ì—°ì‚°) ì‚¬ìš© ì•ˆ í•¨
# cosyvoice = CosyVoice2('pretrained_models/CosyVoice2-0.5B', load_jit=False, load_trt=False, fp16=False)
# # NOTE if you want to reproduce the results on https://funaudiollm.github.io/cosyvoice2, please add text_frontend=False during inference
# # Zero-shot ìŒì„± í•©ì„± (ëª©ì†Œë¦¬ ìƒ˜í”Œ ê¸°ë°˜ ìŒì„± ìƒì„±)
# prompt_speech_16k = load_wav('./asset/zero_shot_prompt.wav', 16000)
# for i, j in enumerate(cosyvoice.inference_zero_shot(
#         'ì¹œêµ¬ê°€ ë©€ë¦¬ì„œ ë³´ë‚´ì¤€ ìƒì¼ ì„ ë¬¼ì„ ë°›ì•˜ì–´. ì˜ˆìƒì¹˜ ëª»í•œ ê¹œì§ ì„ ë¬¼ê³¼ ë”°ëœ»í•œ ì¶•í•˜ ë©”ì‹œì§€ê°€ ë‚´ ë§ˆìŒì„ ê°€ë“ ì±„ì› ê³ , ë¯¸ì†Œê°€ ê½ƒì²˜ëŸ¼ í™œì§ í”¼ì–´ë‚¬ì–´.',  
#         'ì•ìœ¼ë¡œ ë‚˜ë³´ë‹¤ ë” ì˜í•  ìˆ˜ ìˆê¸°ë¥¼ ë°”ë¼!', prompt_speech_16k, stream=False)):
#     torchaudio.save('zero_shot_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)

# # ë¯¸ì„¸ ì œì–´ëœ ìŒì„± í•©ì„± (íŠ¹ì • ë°œìŒ ë° íš¨ê³¼ í¬í•¨) : fine grained control, for supported control, check cosyvoice/tokenizer/tokenizer.py#L248, [laughter] â†’ ë¬¸ì¥ ì¤‘ê°„ì— ì›ƒìŒ ì†Œë¦¬ ì¶”ê°€
# for i, j in enumerate(cosyvoice.inference_cross_lingual('ê·¸ê°€ ê·¸ í™©ë‹¹í•œ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì£¼ëŠ” ë„ì¤‘, ê·¸ëŠ” ê°‘ìê¸° [ì›ƒìŒ] ë©ˆì¶”ì—ˆì–´. ì™œëƒí•˜ë©´ ìŠ¤ìŠ¤ë¡œë„ ë„ˆë¬´ ì›ƒê²¼ê¸° ë•Œë¬¸ì´ì•¼! [ì›ƒìŒ]', prompt_speech_16k, stream=False)):
#     torchaudio.save('fine_grained_control_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)

# # Instruct ëª¨ë“œ (ìŠ¤íƒ€ì¼ ë˜ëŠ” ë°©ì–¸ ë³€ê²½) : instruct usage
# for i, j in enumerate(cosyvoice.inference_instruct2(
#         'ì¹œêµ¬ê°€ ë©€ë¦¬ì„œ ë³´ë‚´ì¤€ ìƒì¼ ì„ ë¬¼ì„ ë°›ì•˜ì–´. ì˜ˆìƒì¹˜ ëª»í•œ ê¹œì§ ì„ ë¬¼ê³¼ ë”°ëœ»í•œ ì¶•í•˜ ë©”ì‹œì§€ê°€ ë‚´ ë§ˆìŒì„ ê°€ë“ ì±„ì› ê³ , ë¯¸ì†Œê°€ ê½ƒì²˜ëŸ¼ í™œì§ í”¼ì–´ë‚¬ì–´.',  
#         'ì´ ë¬¸ì¥ì„ ë¶€ì‚° ì‚¬íˆ¬ë¦¬ë¡œ ë§í•´ì¤˜!', prompt_speech_16k, stream=False)):
#     torchaudio.save('instruct_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)

# # ìŠ¤íŠ¸ë¦¬ë° ê¸°ë°˜ TTS : bistream usage, you can use generator as input, this is useful when using text llm model as input
# # NOTE you should still have some basic sentence split logic because llm can not handle arbitrary sentence length
# def text_generator():
#     yield 'ì¹œêµ¬ê°€ ë©€ë¦¬ì„œ ë³´ë‚´ì¤€ ìƒì¼ ì„ ë¬¼ì„ ë°›ì•˜ì–´.'
#     yield 'ì˜ˆìƒì¹˜ ëª»í•œ ê¹œì§ ì„ ë¬¼ê³¼ ë”°ëœ»í•œ ì¶•í•˜ ë©”ì‹œì§€ê°€'
#     yield 'ë‚´ ë§ˆìŒì„ ê°€ë“ ì±„ì› ê³ ,'
#     yield 'ë¯¸ì†Œê°€ ê½ƒì²˜ëŸ¼ í™œì§ í”¼ì–´ë‚¬ì–´.'

# for i, j in enumerate(cosyvoice.inference_zero_shot(text_generator(), 'ì•ìœ¼ë¡œ ë‚˜ë³´ë‹¤ ë” ì˜í•  ìˆ˜ ìˆê¸°ë¥¼ ë°”ë¼!', prompt_speech_16k, stream=False)):
#     torchaudio.save('zero_shot_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)


# Whisper ëª¨ë¸ ë¡œë“œ
model = whisper.load_model("base", device="cpu")

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
openai.api_key = os.environ['OPENAI_API_KEY']

st.title("ğŸ™ï¸ SelenaAI ")

with st.sidebar.container():
    # st.header("ìŒì„± ì„ íƒ")
    # voice_names = [
    #     "Aria", "Roger", "Sarah", "Laura", "Charlie", "George",
    #     "Callum", "River", "Liam", "Charlotte", "Alice", "Matilda",
    #     "Will", "Jessica", "Eric", "Chris", "Brian", "Daniel", "Lily",
    #     "Bill", "Anna Kim", "Jennie"
    # ]
    # voice_options = {
    #     name: os.environ.get("VOICE_" + name.replace(" ", "_").upper())
    #     for name in voice_names
    # }
    # # í•„í„°ë§: ë“±ë¡ë˜ì§€ ì•Šì€ ìŒì„± ì œê±°
    # voice_options = {name: vid for name, vid in voice_options.items() if vid}
    # if not voice_options:
    #     st.error("ë“±ë¡ëœ ìŒì„± ì˜µì…˜ì´ ì—†ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    #     selected_voice_id = None
    # else:
    #     selected_voice = st.selectbox("ğŸ”Š ìŒì„±ì„ ì„ íƒí•˜ì„¸ìš”", list(voice_options.keys()), key="voice_select") # label_visibility="hidden"
    #     selected_voice_id = voice_options[selected_voice]

    # ì†ë„ ì¡°ì ˆ ìŠ¬ë¼ì´ë”
    tts_speed = st.slider("ìŒì„± ì†ë„ ì¡°ì ˆ", min_value=100, max_value=600, value=180, step=5)
    
    # ì‚¬ìš©ìê°€ ì„ íƒí•  ìˆ˜ ìˆëŠ” TTS ì—”ì§„ ëª©ë¡
    tts_engine = st.selectbox(
        "ì‚¬ìš©í•  TTS ì—”ì§„ì„ ì„ íƒí•˜ì„¸ìš”", 
        ["ì˜¤í”„ë¼ì¸", "ì˜¨ë¼ì¸"])

    # ì„¤ì • ì €ì¥
    st.session_state.tts_engine = tts_engine
    st.session_state.tts_speed = tts_speed



# ì±„íŒ… ê¸°ë¡ ìœ ì§€
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []


# ìŒì„± ë…¹ìŒ í•¨ìˆ˜
def record_audio(duration=6, samplerate=44100):
    st.write("ğŸ¤ ë…¹ìŒ ì¤‘ ì…ë‹ˆë‹¤. ë§ì‘´í•´ì£¼ì„¸ìš”!")
    audio_data = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=1, dtype=np.int16)
    sd.wait()
    temp_audio_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
    write(temp_audio_file.name, samplerate, audio_data)
    return temp_audio_file.name


# ìŒì„± -> í…ìŠ¤íŠ¸ : STT
def speech_to_text(audio_file):
    audio = whisper.load_audio(audio_file)
    result = model.transcribe(audio)
    return result["text"]


# í…ìŠ¤íŠ¸ -> ìŒì„± : TTS
def speak_pyttsx3(text):
    if not text:
        return  # ë¹ˆ í…ìŠ¤íŠ¸ëŠ” ë³€í™˜í•˜ì§€ ì•ŠìŒ
    
    engine = pyttsx3.init()
    voices = engine.getProperty('voices')

    # í•œêµ­ì–´ ìŒì„± ì„ íƒ
    for voice in voices:
        if "Heami" in voice.name or "korean" in voice.name.lower():
            engine.setProperty('voice', voice.id)
            break

    engine.setProperty('rate', 180)
    engine.say(text)
    engine.runAndWait()

# gTTS ìŒì„± ì¶œë ¥ í•¨ìˆ˜
def speak_gtts(text):
    tts = gTTS(text=text, lang='ko', slow=False)
    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
    tts.save(temp_file.name)
    
    # Streamlitì—ì„œ ì˜¤ë””ì˜¤ ì¬ìƒ
    st.audio(temp_file.name, format="audio/mp3")
    os.remove(temp_file.name)

# ìµœì¢… TTS ì‹¤í–‰ í•¨ìˆ˜
def speak(text):
    if tts_engine == "pyttsx3 (ì˜¤í”„ë¼ì¸)":
        speak_pyttsx3(text)
    else:
        speak_gtts(text)

# GPT ì‘ë‹µ ìƒì„±
def ask_gpt(user_input):
    try:
        response = openai.ChatCompletion.create(
            model="gpt-4-turbo", 
            messages=[{"role": "user", "content": user_input}]
        )
        return response["choices"][0]["message"]["content"]
    except Exception as e:
        return f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

# st.set_page_config(layout="centered", initial_sidebar_state="expanded")
# st.title("ğŸ™ï¸ SelenaAI ")


# # ì±„íŒ… UI ì¶œë ¥
# for chat in st.session_state.chat_history:
#     with st.chat_message(chat["role"]):
#         if chat["role"] == "user":
#             st.image(user_img, width=50)
#         else:
#             st.image(ai_img, width=50)
#         st.write(chat["content"])


# ìŒì„± ë…¹ìŒ ë²„íŠ¼
if st.button("ğŸ¤ SelenaAI ì…ë‹ˆë‹¤. ë¬´ì—‡ì´ë“  í¸í•˜ê²Œ ì§ˆë¬¸í•˜ì„¸ìš”"):
    audio_file = record_audio()
    st.success("âœ… ë…¹ìŒ ì™„ë£Œ! ìŒì„± ë³€í™˜ ì¤‘ ì…ë‹ˆë‹¤...")
    text_input = speech_to_text(audio_file)
    st.write(f"ğŸ“ {text_input}")

    response = ask_gpt(text_input)
    st.write(f"ğŸ¤– {response}")

    st.session_state.chat_history.append({"role": "user", "content": text_input})
    st.session_state.chat_history.append({"role": "assistant", "content": response})
    
    speak(response)
    os.remove(audio_file)


st.subheader("ğŸ’¬ ëŒ€í™” ê¸°ë¡")
for chat in st.session_state.chat_history:
    # role = "ğŸ—£ï¸ ì‚¬ìš©ì" if chat["role"] == "user" else "ğŸ¤– ì±—ë´‡"
    # st.write(f"**{role}**: {chat['content']}")

    with st.chat_message(chat["role"]):
        if chat["role"] == "user":
            st.image(user_img, width=50)
        else:
            st.image(ai_img, width=50)
        st.write(chat["content"])


# ìŠ¤í¬ë¡¤ ë²„íŠ¼ ì¶”ê°€ (ë²„íŠ¼ì„ ì˜¤ë¥¸ìª½ í•˜ë‹¨ì— ê³ ì •)
scroll_css = """
    <style>
        .scroll-buttons {
            position: fixed;
            bottom: 20px;
            right: 20px;
            display: flex;
            flex-direction: column;
            gap: 10px;
            z-index: 1000;
        }
        .scroll-buttons button {
            width: 50px;
            height: 50px;
            font-size: 20px;
            border-radius: 50%;
            border: none;
            background-color: #ffffff;
            box-shadow: 2px 2px 10px rgba(0, 0, 0, 0.2);
            cursor: pointer;
        }
        .scroll-buttons button:hover {
            background-color: #f0f0f0;
        }
    </style>
"""
st.markdown(scroll_css, unsafe_allow_html=True)


# JavaScriptë¡œ ìŠ¤í¬ë¡¤ ê¸°ëŠ¥ ì¶”ê°€
scroll_js = """
    <script>
        function scrollToTop() {
            window.scrollTo({top: 0, behavior: 'smooth'});
        }
        function scrollToBottom() {
            window.scrollTo({top: document.body.scrollHeight, behavior: 'smooth'});
        }
    </script>
"""
st.markdown(scroll_js, unsafe_allow_html=True)

# HTMLë¡œ ë²„íŠ¼ ì¶”ê°€ (ì˜¤ë¥¸ìª½ í•˜ë‹¨ì— ê³ ì •)
scroll_buttons_html = """
    <div class="scroll-buttons">
        <button onclick="scrollToTop()">â¬†ï¸</button>
        <button onclick="scrollToBottom()">â¬‡ï¸</button>
    </div>
"""
st.markdown(scroll_buttons_html, unsafe_allow_html=True)

# # ìŠ¤í¬ë¡¤ ë²„íŠ¼ ì¶”ê°€
# col1, col2 = st.columns([1, 1])
# with col1:
#     if st.button("â¬†ï¸"):
#         st.markdown("<script>window.scrollTo(0, 0);</script>", unsafe_allow_html=True)
# with col2:
#     if st.button("â¬‡ï¸"):
#         st.markdown("<script>window.scrollTo(0, document.body.scrollHeight);</script>", unsafe_allow_html=True)


if st.button("ğŸ›‘ ì¢…ë£Œ"):
    st.session_state.chat_history = []
    st.success("ëŒ€í™” ê¸°ë¡ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
