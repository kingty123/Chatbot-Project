# Whisper ì‚¬ìš©
import whisper
import sounddevice as sd
import numpy as np
#import time
import threading                    # ë¹„ë™ê±° ì‘ì—…ì´ë‚˜, ë™ì‹œì— ì—¬ëŸ¬ ì‘ì—… ì²˜ë¦¬
import os
import streamlit as st
import pandas as pd
from dotenv import load_dotenv
import base64
import pyttsx3

from langchain.chat_models import ChatOpenAI
#from langchain_core.pydantic_v1 import BaseModel
from langchain.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings
from langchain.vectorstores import FAISS
from langchain.storage import LocalFileStore
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from PIL import Image
from streamlit_geolocation import streamlit_geolocation



model = whisper.load_model("base", device="cpu")

from langchain_openai import OpenAIEmbeddings

# í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
load_dotenv()

# API í‚¤ë¥¼ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
API_KEY = os.getenv("API_KEY")
os.environ["OPENAI_API_KEY"] = API_KEY 


# Langchainì„ í™œìš©í•˜ê¸° ìœ„í•œ ì„¤ì •ê³¼ RAG ì„¤ì •ì„ ì§„í–‰í•©ë‹ˆë‹¤.
llm = ChatOpenAI(model='gpt-4',
    temperature=0.7)
cache_dir = LocalFileStore("./.cache/practice/")
splitter = CharacterTextSplitter.from_tiktoken_encoder(
    separator="\n",
    chunk_size=600,
    chunk_overlap=100,
)
#retriever = FAISS.load_local("./refer.txt", OpenAIEmbeddings(), allow_dangerous_deserialization=True)


loader = UnstructuredFileLoader("refer.txt")
docs = loader.load_and_split(text_splitter=splitter)
embeddings = OpenAIEmbeddings()
cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
vectorstore = FAISS.from_documents(docs, cached_embeddings)
retriever = vectorstore.as_retriever()



#from streamlit_geolocation import streamlit_geolocation


# Streamlit ì„¸ì…˜ ìƒíƒœë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. ì´ëŠ” ëŒ€í™” ë‚´ì—­ì„ ì €ì¥í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'question' not in st.session_state:
    st.session_state.question = ""
if 'mode' not in st.session_state:
    st.session_state.mode = "voice" # ê¸°ë³¸ ëª¨ë“œ = ìŒì„±

# ì´ë¯¸ì§€ ê²½ë¡œ ì„¤ì •
ai_avatar = "karina_1-removebg-preview.png"  # AI ì•„ë°”íƒ€ ì´ë¯¸ì§€
user_avatar = "ì‚¬ëŒì´ë¯¸ì§€_1.jpg"  # ì‚¬ìš©ì ì•„ë°”íƒ€ ì´ë¯¸ì§€

# ì´ë¯¸ì§€ Base64 ë³€í™˜ í•¨ìˆ˜
def get_image_base64(image_path):
    with open(image_path, "rb") as img_file:
        return base64.b64encode(img_file.read()).decode()



# ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
def ask_gpt(user_question):
    # ì´ì „ ëŒ€í™” ë‚´ì—­ì„ ê¸°ë°˜ìœ¼ë¡œ CHATGPTì—ê²Œ ìš”ì²­í•  ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    conservation_history = "\n".join(st.session_state.chat_history[-50:])

    prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
            ë‹¹ì‹ ì€ 'Selena'ë¼ëŠ” ì°¨ëŸ‰ìš© ì¸ê³µì§€ëŠ¥ ë¹„ì„œì…ë‹ˆë‹¤. ì¹œê·¼í•˜ê³  ìœ ë¨¸ëŸ¬ìŠ¤í•˜ê²Œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ë©°, ìš´ì „ ì¤‘ì—ë„ ì•ˆì „í•˜ê³  ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì´ ì£¼ëœ ì—­í• ì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ í•˜ë©´ ì •í™•í•˜ê³  ëª…í™•í•œ ë‹µë³€ì„ ì£¼ë˜, ìš´ì „ ì¤‘ì—ë„ í¸ì•ˆí•˜ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ê°„ê²°í•˜ê³  ì¹œê·¼í•˜ê²Œ ì‘ë‹µí•´ì£¼ì„¸ìš”. ì•„ë˜ ì§€ì¹¨ì„ ë”°ë¼ì£¼ì„¸ìš”:

            1. Selena or ì…€ë ˆë‚˜ ë¼ê³  ë¶€ë¥´ëŠ” ìŒì„±ì— ë°˜ì‘í•©ë‹ˆë‹¤.
            2. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì‹ ì¤‘í•˜ê²Œ ë¶„ì„í•©ë‹ˆë‹¤.
            3. ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê³  ê°„ê²°í•˜ë©°, ìš´ì „ ì¤‘ì—ë„ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë‹µë³€í•©ë‹ˆë‹¤.
            4. ì§ˆë¬¸ì´ ë³µì¡í•˜ë©´ ì´í•´í•˜ê¸° ì‰¬ìš´ ì‘ì€ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì„¤ëª…í•©ë‹ˆë‹¤.
            5. ì˜ˆì‹œë‚˜ ë¹„ìœ ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°œë…ì„ ì‰½ê²Œ í’€ì–´ ì„¤ëª…í•©ë‹ˆë‹¤.
            6. ë‹µë³€ì„ ì˜ ëª¨ë¥´ê² ë‹¤ë©´ ê·¸ ì‚¬ì‹¤ì„ ì¸ì •í•˜ê³ , ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤.
            7. ì‚¬ìš©ìê°€ ë” ì•Œì•„ë³¼ ìˆ˜ ìˆë„ë¡ í›„ì† ì§ˆë¬¸ì„ ìœ ë„í•˜ê±°ë‚˜ ê´€ë ¨ëœ ì£¼ì œë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
            8. í•­ìƒ ê¸ì •ì ì´ê³  ê²©ë ¤í•˜ëŠ” íƒœë„ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
            9. ìš´ì „ ì¤‘ì—ëŠ” ì•ˆì „ì„ ìµœìš°ì„ ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ ëŒ€í™”ë¥¼ ìœ ë„í•©ë‹ˆë‹¤.
            \n\n
            {context}",
            """
        ),
        ("human", "{question}")
    ]
    )

    # LLM ì²´ì¸ ì‹¤í–‰í–‰
    chain = (
        {
            "context": retriever,
            "question": RunnablePassthrough(),
        }
        | prompt
        | llm
    )
    result = chain.invoke(user_question)
        
    return result.content





# ì§ˆë¬¸ ì…ë ¥ í´ë¦¬ì–´ í•¨ìˆ˜
def clear_input():
    st.session_state.question = ""


# ìŒì„±ì„ í•©ì„±í•˜ëŠ” í•¨ìˆ˜ : TTS
def speak(text):
    engine = pyttsx3.init()
    engine.say(text)
    engine.runAndWait()

from RealtimeSTT import AudioToTextRecorder
import pyautogui




# í‚¤ì›Œë“œ ê°ì§€ í•¨ìˆ˜(ê¸°ë³¸ ì˜ˆì‹œ)
def detect_keyword(audio):
    #return np.random.rand() > 0.97  # 3% í™•ë¥ ë¡œ í‚¤ì›Œë“œ ê°ì§€
    audio = whisper.pad_or_trim(audio)
    mel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)
    result = model.transcribe(mel)

    # ê°ì§€ëœ í…ìŠ¤íŠ¸ì—ì„œ "Hey"ë¥¼ ì°¾ìŒ
    if "Selena" in result['text'].lower():
        return True
    return False




# process_command() : íŠ¹ì • í‚¤ì›Œë“œë¥¼ ê°ì§€í–ˆì„ ë•Œ í˜¸ì¶œë˜ë©°, ì‚¬ìš©ìì˜ ìŒì„± ëª…ë ¹ ì²˜ë¦¬
def process_command(text):
    print(f"ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­: {text}")
    response = respond_to_command(text)
    print(response)
    pyautogui.typewrite(response)           # ì‘ë‹µì„ ì±„íŒ…ì°½ì— ì…ë ¥


# ìˆ˜ì • í•„ìš” : ëª…ë ¹ì— ëŒ€í•œ ì‘ë‹µì„ ì œê³µí•˜ëŠ” í•¨ìˆ˜
def respond_to_command(command):
    # ì˜ˆì‹œ: íŠ¹ì • ëª…ë ¹ì–´ì— ëŒ€í•œ ì‘ë‹µ ì²˜ë¦¬
    if "ë‚ ì”¨" in command:
        response = "ì˜¤ëŠ˜ì˜ ë‚ ì”¨ëŠ” ë§‘ìŠµë‹ˆë‹¤.â˜€ï¸"
    elif "ì‹œê°„" in command:
        response = "í˜„ì¬ ì‹œê°„ì€ 3ì‹œì…ë‹ˆë‹¤.ğŸ•’"
    else:
        response = "ìš”ì²­í•˜ì‹  ë‚´ìš©ì„ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ í•œ ë²ˆ ë§ì”€í•´ì£¼ì„¸ìš” ğŸ¥²"
    return response


def listen_for_audio():
    recorder = AudioToTextRecorder()
    while True:
        recorder.text(process_command)



if __name__ == "__main__":

    # streamlit page configuration
    st.set_page_config(layout="centered", initial_sidebar_state="expanded")
    # í˜ì´ì§€ ì œëª©
    st.title("ì•ˆë…•í•˜ì„¸ìš”. ë‹¹ì‹ ì˜ ì•ˆì „ì„ ì±…ì„ì§ˆ SelenaAI ì…ë‹ˆë‹¤ ğŸ˜Š")

    # CSS ìŠ¤íƒ€ì¼ ì •ì˜
    st.markdown("""
    <style>
    .chat-container {
        display: flex;
        align-items: start;
        margin-bottom: 20px;
        width: 100%;
    }

    .chat-image {
        width: 100px;
        height: 150px;
        border-radius: 10%;
        margin: 0 15px;
        object-fit : cover;
    }
    .chat-image.ai {
        flex-direction: row-reverse;
        align: right;
        align-items: flex-start;
        justify-content: flex-end;
    }
    .chat-message {
        background-color: #f0f2f6;
        padding: 10px;
        border-radius: 10px;
        max-width: 80%;
    }

    .chat-container.ai {
        flex-direction: row-reverse;
        text-align: auto;
    }         
                
    .chat-message.ai {
        margin-right: 0;
        text-align: auto;
    }   

    </style>
    """, unsafe_allow_html=True)


    #ìŒì„± ë° ì±„íŒ…ëª¨ë“œ ì„ íƒ
    # mode = st.radio("ì–´ë–¤ ëª¨ë“œë¥¼ ì›í•˜ì‹­ë‹ˆê¹Œ? : ", ["ğŸ”Š", "âŒ¨ï¸"])
    # st.session_state.mode = mode
    question = st.text_input("ì–¸ì œë“  í¸í•˜ê²Œ ì§ˆë¬¸í•˜ì„¸ìš”", value=st.session_state.question, key="user_input")


    # ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë²„íŠ¼
    if question and (st.button('ë‹µë³€') or question != st.session_state.get('previous_question', '')):
        st.session_state['previous_question'] = question
        answer = ask_gpt(question)  # GPT-3 ëª¨ë¸ì„ í˜¸ì¶œí•˜ì—¬ ë‹µë³€ì„ ë°›ìŠµë‹ˆë‹¤.

        st.session_state.chat_history.append(f"Question: {question}")
        st.session_state.chat_history.append(f"Answer: {answer}")
        st.session_state.question = ""  # ì…ë ¥ í•„ë“œ í´ë¦¬ì–´

        # ìŒì„± ëª¨ë“œì¼ ê²½ìš° ìŒì„±ìœ¼ë¡œ ë‹µë³€ ì½ê¸°
        if st.session_state.mode == "voice":
            speak(answer)

            
        # ëŒ€í™” ë‚´ì—­ í‘œì‹œ
        for message in st.session_state.chat_history:
            formatted_message = message.replace("\n", "<br>")  # ğŸ”¹ f-string ë°”ê¹¥ì—ì„œ ë³€í™˜ ì²˜ë¦¬!

            if message.startswith("Question:"):
                st.markdown(
                    f"""
                    <div class="chat-container">
                        <img src="data:image/jpeg;base64,{get_image_base64(user_avatar)}" class="chat-image">
                        <div class="chat-message">{formatted_message}</div>
                        <div class="button-group">
                            <button class="copy-btn" onclick="copyToClipboard('{formatted_message}')"> âœ”ï¸ </button>
                            <button class="eval-btn" onclick="evaluateResponse('Good')"> ğŸ‘ </button>
                            <button class="eval-btn" onclick="evaluateResponse('Bad')"> ğŸ‘ </button>    
                        </div>
                    </div>
                    """,
                    unsafe_allow_html=True
                )   .        .        .     

            else:
                st.markdown(
                    f"""
                    <div class="chat-container ai">
                        <img src="data:image/jpeg;base64,{get_image_base64(ai_avatar)}" class="chat-image">
                        <div class="chat-message ai">{formatted_message}</div>
                        <div class="button-group">
                            <button class="copy-btn" onclick="copyToClipboard('{formatted_message}')"> âœ”ï¸ </button>
                            <button class="eval-btn" onclick="evaluateResponse('Good')"> ğŸ‘ </button>
                            <button class="eval-btn" onclick="evaluateResponse('Bad')"> ğŸ‘ </button>    
                        </div>
                    </div>
                    """,
                    unsafe_allow_html=True
                )




       # ìë™ ìŠ¤í¬ë¡¤ ì•„ë˜ë¡œ
        st.markdown("<script>window.scrollTo(0, document.body.scrollHeight);</script>", unsafe_allow_html=True)
        # ìë™ ìŠ¤í¬ë¡¤ ìœ„ë¡œ (ì˜ˆ: 100px ìœ„ë¡œ ì´ë™)
        st.markdown("<script>window.scrollBy(0, -100);</script>", unsafe_allow_html=True)

            
    else:
        st.error("Please enter a question.")



    # ìë°”ìŠ¤í¬ë¦½íŠ¸ í•¨ìˆ˜ ì¶”ê°€ (ë³µì‚¬ ë° í‰ê°€ ê¸°ëŠ¥)
    st.markdown("""
    <div id="scroll-top" onclick="window.scrollTo(0,0)">â¬†ï¸</div>
    <div id="scroll-bottom" onclick="window.scrollTo(0,document.body.scrollHeight)">â¬‡ï¸</div>
    <script>
        function copyToClipboard(text) {
            navigator.clipboard.writeText(text).then(() => {
                alert('Copied to clipboard');
            });
        }

        function evaluateResponse(evaluation) {
            alert('You rated this response as: ' + evaluation);
        }
    </script>
    """, unsafe_allow_html=True)



